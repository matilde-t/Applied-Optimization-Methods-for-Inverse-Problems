{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24809ea5",
   "metadata": {},
   "source": [
    "## A handy dandy trick for solvers\n",
    "\n",
    "You have written quite a few algorithms to solve inverse problems. And I ask many questions regarding plot this and\n",
    "plot that. How is convergence of this algorithm, vs that. And of course you can edit the solver to log you that information, but there are (from a software engineering point of view) nicer solutions.\n",
    "\n",
    "Let's write a quick and dirty gradient descent algorithm to showcase that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19008da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46572c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(df, x0, step, iters):\n",
    "    x = np.copy(x0)\n",
    "    for _ in tqdm.trange(iters):\n",
    "        x -= step * df(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9ac56",
   "metadata": {},
   "source": [
    "The function I choose to optimize is the Rosenbrock function:\n",
    "\n",
    "$$\n",
    "f(x,y)=(x−1)^2+b (y−x1^2)^2\n",
    "$$\n",
    "and it's gradient:\n",
    "$$\n",
    "\\nabla f = \\begin{bmatrix} \n",
    "2(x - 1) - 4b(y−x^2) x \\\\\n",
    "2b(y - x^2)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some b\n",
    "b = 10;\n",
    "f = lambda x,y: (x - 1) ** 2 + b * (y - x ** 2) ** 2;\n",
    "df = lambda x,y: np.array([2 * (x - 1) - 4 * b * (y - x ** 2) * x, 2 * b * (y - x ** 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516caf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure \n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Depending on your matplotlib version:\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "# or: \n",
    "# ax = fig.gca(projection='3d')\n",
    "\n",
    "# Evaluate function\n",
    "X = np.arange(-2, 2, 0.15)\n",
    "Y = np.arange(-1, 3, 0.15)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = f(X,Y)\n",
    "\n",
    "# Plot the surface\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.gist_heat_r, linewidth=0, antialiased=False)\n",
    "ax.set_zlim(0, 200)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e09d4",
   "metadata": {},
   "source": [
    "# Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = lambda X: f(X[0],X[1])\n",
    "dF = lambda X: df(X[0],X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([-1.4,1.1])\n",
    "print(F(x0))\n",
    "print(dF(x0))\n",
    "\n",
    "# Initialize figure \n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.contour(X,Y,Z,200)\n",
    "plt.plot([x0[0]],[x0[1]],marker='o',markersize=15, color ='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xprime = gradient_descent(dF, x0, 0.01, 20)\n",
    "\n",
    "# plot start and end point\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.contour(X,Y,Z,200)\n",
    "plt.plot([x0[0]],[x0[1]],marker='o',markersize=15, color ='r')\n",
    "plt.plot([xprime[0]],[xprime[1]],marker='o',markersize=15, color ='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fd10c",
   "metadata": {},
   "source": [
    "Now, if your exercise is to show all the intermediate steps of the algorithm in the grap, you can modify the above gradient desent, to log that and return it as well. But then I ask for the convergence rate, and you need to adjust that as well, let us look at a more general approach: callbacks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54031aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(df, x0, step, iters, callback=None):\n",
    "    x = np.copy(x0)\n",
    "    for i in tqdm.trange(iters):\n",
    "        x -= step * df(x)\n",
    "        \n",
    "        if callback:\n",
    "            callback(np.copy(x), i)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd7409",
   "metadata": {},
   "source": [
    "This is the same algorithm as above, but we take an optional argument, which you can use like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "def callback1(x, i):\n",
    "    xs.append(x)\n",
    "    \n",
    "xprime = gradient_descent(dF, x0, 0.01, 20, callback=callback1)\n",
    "\n",
    "for x in xs:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about a plot of the function values:\n",
    "fs = []\n",
    "def callback1(x, i):\n",
    "    fs.append(F(x))\n",
    "    \n",
    "xprime = gradient_descent(dF, x0, 0.01, 20, callback=callback1)\n",
    "\n",
    "plt.plot(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62185d94",
   "metadata": {},
   "source": [
    "And why pass the iteration number as well? If you run gradient descent for 1000 iterations for a tomographic reconstruction problem, you might not want to save each intermediate example, but only every kth one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about a plot of the function values:\n",
    "fs = []\n",
    "def callback1(x, i):\n",
    "    if i % 10 == 0:\n",
    "        fs.append(F(x))\n",
    "    \n",
    "xprime = gradient_descent(dF, x0, 0.005, 1000, callback=callback1)\n",
    "\n",
    "plt.plot(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ed6ee",
   "metadata": {},
   "source": [
    "The only thing missing is composition of callbacks. Currently, you can only use a single callback each call, but if you want to collect two differrent informations you are screwed. So, as an exercise create a callback class, that is composable, and overload e.g. the `__or__` operator (e.g. the pipe), to make it composable, such that a call like `cb = cb1 | cb2 | cb3` and you can pass it to the solver that way. The expected outcome of that call would be to call each of the three callbacks with the current guess and iteration number. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
